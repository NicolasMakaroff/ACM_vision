
reading train images...
preprocessing train volumes...
cropping train volumes...
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 961996.33it/s]


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 16.25it/s]
  0%|                                                                                                                          | 0/100 [00:00<?, ?it/s]
padding train volumes...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 190.37it/s]



































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 99/100 [01:11<00:00,  1.08it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:13<00:00,  1.37it/s]
 18%|████████████████████▎                                                                                            | 18/100 [00:00<00:01, 71.49it/s]
done creating train dataset

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 49.95it/s]
preprocessing validation volumes...
cropping validation volumes...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 180013.05it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 13.91it/s]
  0%|                                                                                                                           | 0/10 [00:00<?, ?it/s]
padding validation volumes...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 333.30it/s]



 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 9/10 [00:07<00:00,  1.57it/s]
normalizing validation volumes...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.20it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 44.50it/s]
/mnt/nfs/rdata02-users/users/makaroff/UNet2D/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
Epoch 0:   0%|                                                                                                                 | 0/232 [00:00<?, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
   | Name       | Type            | Params
------------------------------------------------
0  | encoder1   | Sequential      | 10.2 K
1  | pool1      | MaxPool2d       | 0
2  | encoder2   | Sequential      | 55.6 K
3  | pool2      | MaxPool2d       | 0
4  | encoder3   | Sequential      | 221 K
5  | pool3      | MaxPool2d       | 0
6  | encoder4   | Sequential      | 885 K
7  | pool4      | MaxPool2d       | 0
8  | bottleneck | Sequential      | 3.5 M
9  | upconv4    | ConvTranspose2d | 524 K
10 | decoder4   | Sequential      | 1.8 M
11 | upconv3    | ConvTranspose2d | 131 K
12 | decoder3   | Sequential      | 442 K
13 | upconv2    | ConvTranspose2d | 32.8 K
14 | decoder2   | Sequential      | 110 K
15 | upconv1    | ConvTranspose2d | 8.2 K
16 | decoder1   | Sequential      | 27.8 K
17 | conv       | Conv2d          | 33
------------------------------------------------
7.8 M     Trainable params
0         Non-trainable params
7.8 M     Total params
31.052    Total estimated model params size (MB)
/mnt/nfs/rdata02-users/users/makaroff/UNet2D/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /mnt/nfs/rdata02-users/users/makaroff/UNet_pytorch/None/version_None/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
/mnt/nfs/rdata02-users/users/makaroff/UNet2D/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/mnt/nfs/rdata02-users/users/makaroff/UNet2D/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.

Epoch 0:   5%|████▊                                                                                       | 12/232 [00:02<00:39,  5.54it/s, loss=0.969]














Epoch 1:   0%|                                                                                         | 0/232 [00:00<?, ?it/s, loss=0.931, v_num=zyxd]
/mnt/nfs/rdata02-users/users/makaroff/UNet2D/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.













Epoch 1:  95%|██████████████████████████████████████████████████████████████████████████▉    | 220/232 [00:27<00:01,  7.89it/s, loss=0.901, v_num=zyxd]








































































Epoch 6:  93%|█████████████████████████████████████████████████████████████████████████▏     | 215/232 [00:27<00:02,  7.69it/s, loss=0.462, v_num=zyxd]

























Epoch 8:  57%|████████████████████████████████████████████▉                                  | 132/232 [00:17<00:13,  7.40it/s, loss=0.397, v_num=zyxd]
/mnt/nfs/rdata02-users/users/makaroff/UNet2D/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")